{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from PhyDNet.models.models import ConvLSTM,PhyCell, EncoderRNN\n",
    "from PhyDNet.data.moving_mnist import MovingMNIST\n",
    "from PhyDNet.constrain_moments import K2M\n",
    "import argparse\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--root', type=str, default='data/')\n",
    "# parser.add_argument('--batch_size', type=int, default=16, help='batch_size')\n",
    "# parser.add_argument('--nepochs', type=int, default=2001, help='nb of epochs')\n",
    "# parser.add_argument('--print_every', type=int, default=1, help='')\n",
    "# parser.add_argument('--eval_every', type=int, default=10, help='')\n",
    "# parser.add_argument('--save_name', type=str, default='phydnet', help='')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "batch_size=32\n",
    "eval_every=1\n",
    "print_every=1\n",
    "nepochs=1\n",
    "data_range = 1.0 # data range 0 to 1 - images normalized this way\n",
    "root='PhyDNet/data/'\n",
    "save_name='phydnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "mm = MovingMNIST(root=root, is_train=False, n_frames_input=10, n_frames_output=10, num_objects=[2])\n",
    "# Split ratio for train and test\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Calculate the lengths of train and test sets\n",
    "train_length = int(train_ratio * len(mm))\n",
    "test_length = len(mm) - train_length\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = random_split(mm, [train_length, test_length])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "constraints = torch.zeros((49,7,7)).to(device)\n",
    "ind = 0\n",
    "for i in range(0,7):\n",
    "    for j in range(0,7):\n",
    "        constraints[ind,i,j] = 1\n",
    "        ind +=1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer  0 input dim  64  hidden dim  128\n",
      "layer  1 input dim  128  hidden dim  128\n",
      "layer  2 input dim  128  hidden dim  64\n",
      "phycell  230803\n",
      "convcell  2508032\n",
      "encoder  3091732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  22%|██▏       | 56/250 [01:28<05:07,  1.58s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 126\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconvcell \u001b[39m\u001b[38;5;124m'\u001b[39m , count_parameters(convcell) ) \n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder \u001b[39m\u001b[38;5;124m'\u001b[39m , count_parameters(encoder) ) \n\u001b[0;32m--> 126\u001b[0m \u001b[43mtrainIters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43meval_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m encoder\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPhyDNet/save/encoder_phydnet.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    129\u001b[0m encoder\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[5], line 52\u001b[0m, in \u001b[0;36mtrainIters\u001b[0;34m(encoder, nepochs, print_every, eval_every, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     51\u001b[0m target_tensor \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 52\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m)\u001b[49m                                   \n\u001b[1;32m     53\u001b[0m loss_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     54\u001b[0m i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36mtrain_on_batch\u001b[0;34m(input_tensor, target_tensor, encoder, encoder_optimizer, criterion, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ei \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(input_length\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m): \n\u001b[0;32m----> 9\u001b[0m     encoder_output, encoder_hidden, output_image,_,_ \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mei\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mei\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m criterion(output_image,input_tensor[:,ei\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,:,:,:])\n\u001b[1;32m     12\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m input_tensor[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:,:,:] \u001b[38;5;66;03m# first decoder input = last image of input sequence\u001b[39;00m\n",
      "File \u001b[0;32m/compass/Shared/Users/bogdanov/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/compass/Shared/Users/bogdanov/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/compass/Shared/Users/bogdanov/vyzkumny_ukol/PhyDNet/models/models.py:271\u001b[0m, in \u001b[0;36mEncoderRNN.forward\u001b[0;34m(self, input, first_timestep, decoding)\u001b[0m\n\u001b[1;32m    268\u001b[0m input_conv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_Er(\u001b[38;5;28minput\u001b[39m)     \n\u001b[1;32m    270\u001b[0m hidden1, output1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphycell(input_phys, first_timestep)\n\u001b[0;32m--> 271\u001b[0m hidden2, output2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvcell\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_conv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_timestep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m decoded_Dp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_Dp(output1[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    274\u001b[0m decoded_Dr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_Dr(output2[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/compass/Shared/Users/bogdanov/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/compass/Shared/Users/bogdanov/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/compass/Shared/Users/bogdanov/vyzkumny_ukol/PhyDNet/models/models.py:146\u001b[0m, in \u001b[0;36mConvLSTM.forward\u001b[0;34m(self, input_, first_timestep)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j,cell \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_list):\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m j\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# bottom layer\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH[j], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC[j] \u001b[38;5;241m=\u001b[39m \u001b[43mcell\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mH\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH[j], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC[j] \u001b[38;5;241m=\u001b[39m cell(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH[j\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH[j],\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC[j]))\n",
      "File \u001b[0;32m/compass/Shared/Users/bogdanov/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/compass/Shared/Users/bogdanov/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/compass/Shared/Users/bogdanov/vyzkumny_ukol/PhyDNet/models/models.py:105\u001b[0m, in \u001b[0;36mConvLSTM_Cell.forward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m    102\u001b[0m h_cur, c_cur \u001b[38;5;241m=\u001b[39m hidden\n\u001b[1;32m    104\u001b[0m combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, h_cur], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# concatenate along channel axis\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m combined_conv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m(combined)\n\u001b[1;32m    106\u001b[0m cc_i, cc_f, cc_o, cc_g \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(combined_conv, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \n\u001b[1;32m    107\u001b[0m i \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(cc_i)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def train_on_batch(input_tensor, target_tensor, encoder, encoder_optimizer, criterion,teacher_forcing_ratio):                \n",
    "    encoder_optimizer.zero_grad()\n",
    "    # input_tensor : torch.Size([batch_size, input_length, channels, cols, rows])\n",
    "    input_length  = input_tensor.size(1)\n",
    "    target_length = target_tensor.size(1)\n",
    "    loss = 0\n",
    "    for ei in range(input_length-1): \n",
    "        encoder_output, encoder_hidden, output_image,_,_ = encoder(input_tensor[:,ei,:,:,:], (ei==0) )\n",
    "        loss += criterion(output_image,input_tensor[:,ei+1,:,:,:])\n",
    "\n",
    "    decoder_input = input_tensor[:,-1,:,:,:] # first decoder input = last image of input sequence\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False \n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden, output_image,_,_ = encoder(decoder_input)\n",
    "        target = target_tensor[:,di,:,:,:]\n",
    "        loss += criterion(output_image,target)\n",
    "        if use_teacher_forcing:\n",
    "            decoder_input = target # Teacher forcing    \n",
    "        else:\n",
    "            decoder_input = output_image\n",
    "\n",
    "    # Moment regularization  # encoder.phycell.cell_list[0].F.conv1.weight # size (nb_filters,in_channels,7,7)\n",
    "    k2m = K2M([7,7]).to(device)\n",
    "    for b in range(0,encoder.phycell.cell_list[0].input_dim):\n",
    "        filters = encoder.phycell.cell_list[0].F.conv1.weight[:,b,:,:] # (nb_filters,7,7)     \n",
    "        m = k2m(filters.double()) \n",
    "        m  = m.float()   \n",
    "        loss += criterion(m, constraints) # constrains is a precomputed matrix   \n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    return loss.item() / target_length\n",
    "\n",
    "\n",
    "def trainIters(encoder, nepochs, print_every=1,eval_every=1,name=''):\n",
    "    train_losses = []\n",
    "    best_mse = float('inf')\n",
    "\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(),lr=0.001)\n",
    "    scheduler_enc = ReduceLROnPlateau(encoder_optimizer, mode='min', patience=2,factor=0.1,verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    loss_epoch = 0\n",
    "    for epoch in range(0, nepochs):\n",
    "        t0 = time.time()\n",
    "        loss_epoch = 0\n",
    "        teacher_forcing_ratio = np.maximum(0 , 1 - epoch * 0.003) \n",
    "        i=0\n",
    "        for out in tqdm(train_loader, desc='train'):\n",
    "            input_tensor = out[1].to(device)\n",
    "            target_tensor = out[2].to(device)\n",
    "            loss = train_on_batch(input_tensor, target_tensor, encoder, encoder_optimizer, criterion, teacher_forcing_ratio)                                   \n",
    "            loss_epoch += loss\n",
    "            i+=1\n",
    "                      \n",
    "        train_losses.append(loss_epoch)        \n",
    "        if (epoch+1) % print_every == 0:\n",
    "            print('epoch ',epoch,  ' loss ',loss_epoch, ' time epoch ',time.time()-t0)\n",
    "            \n",
    "        if (epoch+1) % eval_every == 0:\n",
    "            mse, mae,ssim = evaluate(encoder,test_loader) \n",
    "            scheduler_enc.step(mse)                   \n",
    "            torch.save(encoder.state_dict(),'PhyDNet/save/encoder_{}.pth'.format(name))                           \n",
    "    return train_losses\n",
    "\n",
    "    \n",
    "def evaluate(encoder,loader):\n",
    "    total_mse, total_mae,total_ssim,total_bce = 0,0,0,0\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        i=0\n",
    "        for out in tqdm(loader, desc=f'eval. mse: {total_mse/len(loader):.2f}, mae: {total_mae/len(loader):.2f}, ssim: {total_ssim/len(loader):.2f}'):\n",
    "\n",
    "            input_tensor = out[1].to(device)\n",
    "            target_tensor = out[2].to(device)\n",
    "            input_length = input_tensor.size()[1]\n",
    "            target_length = target_tensor.size()[1]\n",
    "\n",
    "            for ei in range(input_length-1):\n",
    "                encoder_output, encoder_hidden, _,_,_  = encoder(input_tensor[:,ei,:,:,:], (ei==0))\n",
    "\n",
    "            decoder_input = input_tensor[:,-1,:,:,:] # first decoder input= last image of input sequence\n",
    "            predictions = []\n",
    "\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden, output_image,_,_ = encoder(decoder_input, False, False)\n",
    "                decoder_input = output_image\n",
    "                predictions.append(output_image.cpu())\n",
    "\n",
    "            input = input_tensor.cpu().numpy()\n",
    "            target = target_tensor.cpu().numpy()\n",
    "            predictions =  np.stack(predictions) # (10, batch_size, 1, 64, 64)\n",
    "            predictions = predictions.swapaxes(0,1)  # (batch_size,10, 1, 64, 64)\n",
    "\n",
    "            mse_batch = np.mean((predictions-target)**2 , axis=(0,1,2)).sum()\n",
    "            mae_batch = np.mean(np.abs(predictions-target) ,  axis=(0,1,2)).sum() \n",
    "            total_mse += mse_batch\n",
    "            total_mae += mae_batch\n",
    "            \n",
    "            for a in range(0,target.shape[0]):\n",
    "                for b in range(0,target.shape[1]):\n",
    "                    total_ssim += ssim(target[a,b,0,], predictions[a,b,0,], data_range=data_range) / (target.shape[0]*target.shape[1]) \n",
    "\n",
    "            \n",
    "            cross_entropy = -target*np.log(predictions) - (1-target) * np.log(1-predictions)\n",
    "            cross_entropy = cross_entropy.sum()\n",
    "            cross_entropy = cross_entropy / (batch_size*target_length)\n",
    "            total_bce +=  cross_entropy\n",
    "            i+=1\n",
    "     \n",
    "    print('eval mse ', total_mse/len(loader),  ' eval mae ', total_mae/len(loader),' eval ssim ',total_ssim/len(loader), ' time= ', time.time()-t0)        \n",
    "    return total_mse/len(loader),  total_mae/len(loader), total_ssim/len(loader)\n",
    "\n",
    "\n",
    "phycell  =  PhyCell(input_shape=(16,16), input_dim=64, F_hidden_dims=[49], n_layers=1, kernel_size=(7,7), device=device) \n",
    "convcell =  ConvLSTM(input_shape=(16,16), input_dim=64, hidden_dims=[128,128,64], n_layers=3, kernel_size=(3,3), device=device)   \n",
    "encoder  = EncoderRNN(phycell, convcell, device)\n",
    "  \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "   \n",
    "print('phycell ' , count_parameters(phycell) )    \n",
    "print('convcell ' , count_parameters(convcell) ) \n",
    "print('encoder ' , count_parameters(encoder) ) \n",
    "\n",
    "trainIters(encoder,nepochs,print_every=print_every,eval_every=1,name=save_name)\n",
    "\n",
    "encoder.load_state_dict(torch.load('PhyDNet/save/encoder_phydnet.pth'))\n",
    "encoder.eval()\n",
    "mse, mae,ssim = evaluate(encoder,test_loader) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 1, 64, 64])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "next(iter(test_loader))[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bogdanov_VU_kernel 3.8.10",
   "language": "python",
   "name": "bogdanov_vu_3.8.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
