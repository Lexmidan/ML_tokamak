{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from PhyDNet.models.models import ConvLSTM,PhyCell, EncoderRNN\n",
    "from PhyDNet.data.moving_mnist import MovingMNIST\n",
    "from PhyDNet.constrain_moments import K2M\n",
    "import argparse\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--root', type=str, default='data/')\n",
    "# parser.add_argument('--batch_size', type=int, default=16, help='batch_size')\n",
    "# parser.add_argument('--nepochs', type=int, default=2001, help='nb of epochs')\n",
    "# parser.add_argument('--print_every', type=int, default=1, help='')\n",
    "# parser.add_argument('--eval_every', type=int, default=10, help='')\n",
    "# parser.add_argument('--save_name', type=str, default='phydnet', help='')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "batch_size=32\n",
    "eval_every=1\n",
    "print_every=1\n",
    "nepochs=1\n",
    "data_range = 1.0 # data range 0 to 1 - images normalized this way\n",
    "root='PhyDNet/data/'\n",
    "save_name='phydnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "mm = MovingMNIST(root=root, is_train=False, n_frames_input=10, n_frames_output=10, num_objects=[2])\n",
    "# Split ratio for train and test\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Calculate the lengths of train and test sets\n",
    "train_length = int(train_ratio * len(mm))\n",
    "test_length = len(mm) - train_length\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = random_split(mm, [train_length, test_length])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "constraints = torch.zeros((49,7,7)).to(device)\n",
    "ind = 0\n",
    "for i in range(0,7):\n",
    "    for j in range(0,7):\n",
    "        constraints[ind,i,j] = 1\n",
    "        ind +=1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer  0 input dim  64  hidden dim  128\n",
      "layer  1 input dim  128  hidden dim  128\n",
      "layer  2 input dim  128  hidden dim  64\n",
      "phycell  230803\n",
      "convcell  2508032\n",
      "encoder  3091732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  14%|█▍        | 35/250 [09:29<58:19, 16.28s/it]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 126\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconvcell \u001b[39m\u001b[38;5;124m'\u001b[39m , count_parameters(convcell) ) \n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder \u001b[39m\u001b[38;5;124m'\u001b[39m , count_parameters(encoder) ) \n\u001b[0;32m--> 126\u001b[0m \u001b[43mtrainIters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43meval_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m encoder\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPhyDNet/save/encoder_phydnet.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    129\u001b[0m encoder\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[6], line 52\u001b[0m, in \u001b[0;36mtrainIters\u001b[0;34m(encoder, nepochs, print_every, eval_every, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     51\u001b[0m target_tensor \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 52\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m)\u001b[49m                                   \n\u001b[1;32m     53\u001b[0m loss_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     54\u001b[0m i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[6], line 33\u001b[0m, in \u001b[0;36mtrain_on_batch\u001b[0;34m(input_tensor, target_tensor, encoder, encoder_optimizer, criterion, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     31\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     32\u001b[0m encoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m target_length\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def train_on_batch(input_tensor, target_tensor, encoder, encoder_optimizer, criterion,teacher_forcing_ratio):                \n",
    "    encoder_optimizer.zero_grad()\n",
    "    # input_tensor : torch.Size([batch_size, input_length, channels, cols, rows])\n",
    "    input_length  = input_tensor.size(1)\n",
    "    target_length = target_tensor.size(1)\n",
    "    loss = 0\n",
    "    for ei in range(input_length-1): \n",
    "        encoder_output, encoder_hidden, output_image,_,_ = encoder(input_tensor[:,ei,:,:,:], (ei==0) )\n",
    "        loss += criterion(output_image,input_tensor[:,ei+1,:,:,:])\n",
    "\n",
    "    decoder_input = input_tensor[:,-1,:,:,:] # first decoder input = last image of input sequence\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False \n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden, output_image,_,_ = encoder(decoder_input)\n",
    "        target = target_tensor[:,di,:,:,:]\n",
    "        loss += criterion(output_image,target)\n",
    "        if use_teacher_forcing:\n",
    "            decoder_input = target # Teacher forcing    \n",
    "        else:\n",
    "            decoder_input = output_image\n",
    "\n",
    "    # Moment regularization  # encoder.phycell.cell_list[0].F.conv1.weight # size (nb_filters,in_channels,7,7)\n",
    "    k2m = K2M([7,7]).to(device)\n",
    "    for b in range(0,encoder.phycell.cell_list[0].input_dim):\n",
    "        filters = encoder.phycell.cell_list[0].F.conv1.weight[:,b,:,:] # (nb_filters,7,7)     \n",
    "        m = k2m(filters.double()) \n",
    "        m  = m.float()   \n",
    "        loss += criterion(m, constraints) # constrains is a precomputed matrix   \n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    return loss.item() / target_length\n",
    "\n",
    "\n",
    "def trainIters(encoder, nepochs, print_every=1,eval_every=1,name=''):\n",
    "    train_losses = []\n",
    "    best_mse = float('inf')\n",
    "\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(),lr=0.001)\n",
    "    scheduler_enc = ReduceLROnPlateau(encoder_optimizer, mode='min', patience=2,factor=0.1,verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    loss_epoch = 0\n",
    "    for epoch in range(0, nepochs):\n",
    "        t0 = time.time()\n",
    "        loss_epoch = 0\n",
    "        teacher_forcing_ratio = np.maximum(0 , 1 - epoch * 0.003) \n",
    "        i=0\n",
    "        for out in tqdm(train_loader, desc='train'):\n",
    "            input_tensor = out[1].to(device)\n",
    "            target_tensor = out[2].to(device)\n",
    "            loss = train_on_batch(input_tensor, target_tensor, encoder, encoder_optimizer, criterion, teacher_forcing_ratio)                                   \n",
    "            loss_epoch += loss\n",
    "            i+=1\n",
    "                      \n",
    "        train_losses.append(loss_epoch)        \n",
    "        if (epoch+1) % print_every == 0:\n",
    "            print('epoch ',epoch,  ' loss ',loss_epoch, ' time epoch ',time.time()-t0)\n",
    "            \n",
    "        if (epoch+1) % eval_every == 0:\n",
    "            mse, mae,ssim = evaluate(encoder,test_loader) \n",
    "            scheduler_enc.step(mse)                   \n",
    "            torch.save(encoder.state_dict(),'PhyDNet/save/encoder_{}.pth'.format(name))                           \n",
    "    return train_losses\n",
    "\n",
    "    \n",
    "def evaluate(encoder,loader):\n",
    "    total_mse, total_mae,total_ssim,total_bce = 0,0,0,0\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        i=0\n",
    "        for out in tqdm(loader, desc=f'eval. mse: {total_mse/len(loader):.2f}, mae: {total_mae/len(loader):.2f}, ssim: {total_ssim/len(loader):.2f}'):\n",
    "\n",
    "            input_tensor = out[1].to(device)\n",
    "            target_tensor = out[2].to(device)\n",
    "            input_length = input_tensor.size()[1]\n",
    "            target_length = target_tensor.size()[1]\n",
    "\n",
    "            for ei in range(input_length-1):\n",
    "                encoder_output, encoder_hidden, _,_,_  = encoder(input_tensor[:,ei,:,:,:], (ei==0))\n",
    "\n",
    "            decoder_input = input_tensor[:,-1,:,:,:] # first decoder input= last image of input sequence\n",
    "            predictions = []\n",
    "\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden, output_image,_,_ = encoder(decoder_input, False, False)\n",
    "                decoder_input = output_image\n",
    "                predictions.append(output_image.cpu())\n",
    "\n",
    "            input = input_tensor.cpu().numpy()\n",
    "            target = target_tensor.cpu().numpy()\n",
    "            predictions =  np.stack(predictions) # (10, batch_size, 1, 64, 64)\n",
    "            predictions = predictions.swapaxes(0,1)  # (batch_size,10, 1, 64, 64)\n",
    "\n",
    "            mse_batch = np.mean((predictions-target)**2 , axis=(0,1,2)).sum()\n",
    "            mae_batch = np.mean(np.abs(predictions-target) ,  axis=(0,1,2)).sum() \n",
    "            total_mse += mse_batch\n",
    "            total_mae += mae_batch\n",
    "            \n",
    "            for a in range(0,target.shape[0]):\n",
    "                for b in range(0,target.shape[1]):\n",
    "                    total_ssim += ssim(target[a,b,0,], predictions[a,b,0,], data_range=data_range) / (target.shape[0]*target.shape[1]) \n",
    "\n",
    "            \n",
    "            cross_entropy = -target*np.log(predictions) - (1-target) * np.log(1-predictions)\n",
    "            cross_entropy = cross_entropy.sum()\n",
    "            cross_entropy = cross_entropy / (batch_size*target_length)\n",
    "            total_bce +=  cross_entropy\n",
    "            i+=1\n",
    "     \n",
    "    print('eval mse ', total_mse/len(loader),  ' eval mae ', total_mae/len(loader),' eval ssim ',total_ssim/len(loader), ' time= ', time.time()-t0)        \n",
    "    return total_mse/len(loader),  total_mae/len(loader), total_ssim/len(loader)\n",
    "\n",
    "\n",
    "phycell  =  PhyCell(input_shape=(16,16), input_dim=64, F_hidden_dims=[49], n_layers=1, kernel_size=(7,7), device=device) \n",
    "convcell =  ConvLSTM(input_shape=(16,16), input_dim=64, hidden_dims=[128,128,64], n_layers=3, kernel_size=(3,3), device=device)   \n",
    "encoder  = EncoderRNN(phycell, convcell, device)\n",
    "  \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "   \n",
    "print('phycell ' , count_parameters(phycell) )    \n",
    "print('convcell ' , count_parameters(convcell) ) \n",
    "print('encoder ' , count_parameters(encoder) ) \n",
    "\n",
    "trainIters(encoder,nepochs,print_every=print_every,eval_every=1,name=save_name)\n",
    "\n",
    "encoder.load_state_dict(torch.load('PhyDNet/save/encoder_phydnet.pth'))\n",
    "encoder.eval()\n",
    "mse, mae,ssim = evaluate(encoder,test_loader) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bogdanov_VU_kernel 3.8.10",
   "language": "python",
   "name": "bogdanov_vu_3.8.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
