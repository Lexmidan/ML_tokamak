{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "import torchsummary\n",
    "from torch.optim import lr_scheduler\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "import torch.nn as nn\n",
    "%matplotlib inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\", \"pdf\")  # For export\n",
    "sns.reset_orig()\n",
    "sns.set()\n",
    "\n",
    "# Tensorboard extension (for visualization purposes later)\n",
    "%load_ext tensorboard\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shots = [16769, 16773]#, 16534, 16769, 16773, 18057]\n",
    "shot_df = pd.DataFrame([])\n",
    "for shot in shots:\n",
    "    df = pd.read_csv(f'/compass/Shared/Users/bogdanov/vyzkumny_ukol/LHmode-detection-shot{shot}.csv')\n",
    "    shot_df = pd.concat([shot_df, df], axis=0)\n",
    "\n",
    "df_mode = shot_df['mode'].copy()\n",
    "df_mode[shot_df['mode']=='L-mode']=0\n",
    "df_mode[shot_df['mode']=='H-mode']=1\n",
    "df_mode[shot_df['mode']=='ELM']=2\n",
    "shot_df['mode'] = df_mode\n",
    "shot_df = shot_df.reset_index(drop=True) #each shot has its own indexing starting from 0 to 2232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, annotations, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = annotations #pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.loc[idx, 'filename']) #proc ne 'filename'?\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "    \n",
    "dataset = ImageDataset(annotations=shot_df[['filename', 'mode']], img_dir='/compass/Shared/Users/bogdanov/vyzkumny_ukol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 1.\n",
    "# Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``.\n",
    "model_ft.fc = torch.nn.Linear(num_ftrs, 1)\n",
    "\n",
    "\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here I simply print the structure of model for input with size (3,500,640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torchsummary.summary(model_ft, (3,500,640))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I will try to freeze all layers except 4-th and fc layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, para in model_ft.named_parameters():\n",
    "#     print(\"=\"*40)\n",
    "#     print(f\"name: {name}\")\n",
    "#     print(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, para in model_ft.named_parameters():\n",
    "    if \"layer4\" in name or \"fc\" in name:\n",
    "        continue\n",
    "    else:\n",
    "        para.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateCallback(Callback):\n",
    "    def __init__(self, input_imgs, every_n_epochs=1):\n",
    "        super().__init__()\n",
    "        self.input_imgs = input_imgs  # Images to reconstruct during training\n",
    "        # Only save those images every N epochs (otherwise tensorboard gets quite large)\n",
    "        self.every_n_epochs = every_n_epochs\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
    "            # Reconstruct images\n",
    "            input_imgs = self.input_imgs.to(pl_module.device)\n",
    "            with torch.no_grad():\n",
    "                pl_module.eval()\n",
    "                reconst_imgs = pl_module(input_imgs)\n",
    "                pl_module.train()\n",
    "            # Plot and add to tensorboard\n",
    "            imgs = torch.stack([input_imgs, reconst_imgs], dim=1).flatten(0, 1)\n",
    "            grid = torchvision.utils.make_grid(imgs, nrow=2, normalize=True, range=(-1, 1))\n",
    "            trainer.logger.experiment.add_image(\"Reconstructions\", grid, global_step=trainer.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, val_loader):\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        max_epochs=500,\n",
    "        \n",
    "    )\n",
    "    trainer.fit(model, train_loader, test_loader)\n",
    "    trainer.logger._log_graph = True  # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, dataloaders=val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "    result = {\"test\": test_result, \"val\": val_result}\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `ResNet`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/compass/Shared/Users/bogdanov/vyzkumny_ukol/ResnetEncoder.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226a7570797465726875622e746f6b2e6970702e6361732e637a222c2275736572223a22626f6764616e6f76227d/compass/Shared/Users/bogdanov/vyzkumny_ukol/ResnetEncoder.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trained_model, result \u001b[39m=\u001b[39m train(model_ft, train_dataloader, test_dataloader, val_dataloader)\n",
      "\u001b[1;32m/compass/Shared/Users/bogdanov/vyzkumny_ukol/ResnetEncoder.ipynb Cell 16\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226a7570797465726875622e746f6b2e6970702e6361732e637a222c2275736572223a22626f6764616e6f76227d/compass/Shared/Users/bogdanov/vyzkumny_ukol/ResnetEncoder.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(model, train_loader, test_loader, val_loader):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226a7570797465726875622e746f6b2e6970702e6361732e637a222c2275736572223a22626f6764616e6f76227d/compass/Shared/Users/bogdanov/vyzkumny_ukol/ResnetEncoder.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# Create a PyTorch Lightning trainer with the generation callback\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226a7570797465726875622e746f6b2e6970702e6361732e637a222c2275736572223a22626f6764616e6f76227d/compass/Shared/Users/bogdanov/vyzkumny_ukol/ResnetEncoder.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226a7570797465726875622e746f6b2e6970702e6361732e637a222c2275736572223a22626f6764616e6f76227d/compass/Shared/Users/bogdanov/vyzkumny_ukol/ResnetEncoder.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226a7570797465726875622e746f6b2e6970702e6361732e637a222c2275736572223a22626f6764616e6f76227d/compass/Shared/Users/bogdanov/vyzkumny_ukol/ResnetEncoder.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         devices\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226a7570797465726875622e746f6b2e6970702e6361732e637a222c2275736572223a22626f6764616e6f76227d/compass/Shared/Users/bogdanov/vyzkumny_ukol/ResnetEncoder.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         max_epochs\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226a7570797465726875622e746f6b2e6970702e6361732e637a222c2275736572223a22626f6764616e6f76227d/compass/Shared/Users/bogdanov/vyzkumny_ukol/ResnetEncoder.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226a7570797465726875622e746f6b2e6970702e6361732e637a222c2275736572223a22626f6764616e6f76227d/compass/Shared/Users/bogdanov/vyzkumny_ukol/ResnetEncoder.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     )\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226a7570797465726875622e746f6b2e6970702e6361732e637a222c2275736572223a22626f6764616e6f76227d/compass/Shared/Users/bogdanov/vyzkumny_ukol/ResnetEncoder.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     trainer\u001b[39m.\u001b[39;49mfit(model, train_loader, test_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226a7570797465726875622e746f6b2e6970702e6361732e637a222c2275736572223a22626f6764616e6f76227d/compass/Shared/Users/bogdanov/vyzkumny_ukol/ResnetEncoder.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     trainer\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39m_log_graph \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m  \u001b[39m# If True, we plot the computation graph in tensorboard\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226a7570797465726875622e746f6b2e6970702e6361732e637a222c2275736572223a22626f6764616e6f76227d/compass/Shared/Users/bogdanov/vyzkumny_ukol/ResnetEncoder.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     trainer\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39m_default_hp_metric \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# Optional logging argument that we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/compass/Shared/Users/bogdanov/.venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\n\u001b[1;32m    506\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    507\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m     ckpt_path: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    512\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    513\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Runs the full optimization routine.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \n\u001b[1;32m    515\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    537\u001b[0m \n\u001b[1;32m    538\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m     model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    540\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m    541\u001b[0m     _verify_strategy_supports_compile(model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy)\n",
      "File \u001b[0;32m/compass/Shared/Users/bogdanov/.venv/lib/python3.8/site-packages/pytorch_lightning/utilities/compile.py:131\u001b[0m, in \u001b[0;36m_maybe_unwrap_optimized\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[1;32m    130\u001b[0m _check_mixed_imports(model)\n\u001b[0;32m--> 131\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    132\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(model)\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `ResNet`"
     ]
    }
   ],
   "source": [
    "trained_model, result = train(model_ft, train_dataloader, test_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
