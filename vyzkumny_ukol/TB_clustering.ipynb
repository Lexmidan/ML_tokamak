{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard clustering\n",
    "### Here embeddings needed to calculate t-SNE, PCA and UMAP in TB are created\n",
    "- Later code from this notebook will be integrated to `test_model.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import confinement_mode_classifier as cmc\n",
    "from datetime import datetime\n",
    "import time \n",
    "import torchvision\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import re\n",
    "\n",
    "path = Path(os.getcwd())\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_path = f'{path}/data/LH_alpha'\n",
    "file_names = os.listdir(data_dir_path)\n",
    "\n",
    "\n",
    "#Chose what shots will be used in notebook. Removed shots has different dimensions\n",
    "batch_size = 32\n",
    "shot_numbers = [re.search(r'shot_(\\d+)', file_name).group(1) for file_name in file_names]\n",
    "removed_shots = ['13182', '20009','20112', '20143', '20145', '20146', '20147', '16987', '20144']\n",
    "shot_numbers = [valid_shot for valid_shot in shot_numbers if valid_shot not in removed_shots]\n",
    "\n",
    "\n",
    "shot_df, _, _, _ = cmc.load_and_split_dataframes(path, shot_numbers, shots_for_testing=[], shots_for_validation=[])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pretrained_model = torchvision.models.resnet18(weights='IMAGENET1K_V1', )\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = pretrained_model.fc.in_features\n",
    "pretrained_model.fc = nn.Linear(num_ftrs, 3) #3 classes: L-mode, H-mode, ELM\n",
    "pretrained_model = pretrained_model.to(device)\n",
    "\n",
    "model_path = f'{path}/runs/15-01-2024, 01-03-15 Ensembled_2xris1_models_classifier_training_classifier_training_all_layers'\n",
    "\n",
    "#TEST one camera model\n",
    "# pretrained_model.load_state_dict(torch.load(f'{model_path}/model_fc_trained.pt'))\n",
    "# pretrained_model.eval()\n",
    "# pretrained_model.to(device)\n",
    "# test_dataloader = cmc.get_dloader(test_df, path=path, batch_size=batch_size, shuffle=False, balance_data=False)\n",
    "\n",
    "\n",
    "#TEST ensembled model\n",
    "ensembled_model = cmc.TwoImagesModel(modelA=pretrained_model, modelB=pretrained_model, hidden_units=30).to(device)\n",
    "ensembled_model.load_state_dict(torch.load(f'{model_path}/model.pt', map_location=torch.device('cpu')))\n",
    "ensembled_model.eval()\n",
    "ensembled_model.to(device)\n",
    "\n",
    "test_dataloader = cmc.get_dloader(shot_df, path=path, batch_size=batch_size, shuffle=False, \n",
    "                                  balance_data=True, second_img_opt='RIS1', ris_option='RIS1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove classifier in order to study higher dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembled_model.classifier = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Get the current device (assuming one GPU is available)\n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    # Print GPU properties\n",
    "    print(torch.cuda.get_device_properties(device))\n",
    "\n",
    "    # Get GPU memory information\n",
    "    print(f\"Total GPU memory: {torch.cuda.get_device_properties(device).total_memory / 1e9} GB\")\n",
    "    print(f\"Allocated GPU memory: {torch.cuda.memory_allocated(device) / 1e9} GB\")\n",
    "    print(f\"Reserved GPU memory: {torch.cuda.memory_reserved(device) / 1e9} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7812b86c8c240c681b6af02267ebf34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding images:   0%|          | 0/1084 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def embed_imgs(model, data_loader, max_iter):\n",
    "    # Encode all images in the data_laoder using model, and return both images and encodings\n",
    "    img_list, embed_list, mode_list = [], [], []\n",
    "    model.eval()\n",
    "    batch_number = 0\n",
    "    for batch in tqdm(data_loader, desc=\"Encoding images\", leave=False, ):\n",
    "        batch_number+=1\n",
    "        with torch.no_grad():\n",
    "            z = model(batch['img'].to(device).float())\n",
    "        img_list.append(F.interpolate(batch['img'][:, 0], size=(250, 320), mode='bilinear', align_corners=False))\n",
    "        embed_list.append(z)\n",
    "        mode_list.append(batch['label'])\n",
    "\n",
    "        if batch_number > max_iter:\n",
    "            break\n",
    "\n",
    "    replacement_map = {0: 'L-mode', 1: 'H-mode', 2: 'ELM'}\n",
    "    new_list = [replacement_map[value.item()] for value in torch.cat(mode_list)]\n",
    "    return {'img':torch.cat(img_list, dim=0), 'output': torch.cat(embed_list, dim=0), 'label': new_list}\n",
    "\n",
    "img_embeds = embed_imgs(ensembled_model, test_dataloader, max_iter=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "writer = SummaryWriter(f\"{path}/runs/15-01-2024, 01-03-15 Ensembled_2xris1_models_classifier_training_classifier_training_all_layers/tsne clustering\")\n",
    "\n",
    "import numpy as np\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "NUM_IMGS = 625\n",
    "writer.add_embedding(img_embeds['output'][:NUM_IMGS], # Encodings per image\n",
    "                     metadata=img_embeds['label'][:NUM_IMGS], # Adding the labels per image to the plot\n",
    "                     label_img=(img_embeds['img'][:NUM_IMGS]*(std[:, None, None]) + 2*mean[:, None, None])) \n",
    "                    #I intentionally adding 5 times mean, because in tensorboard the color imaging works poorly with dimmer imgs\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bogdanov_VU_kernel 3.8.10",
   "language": "python",
   "name": "bogdanov_vu_3.8.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
