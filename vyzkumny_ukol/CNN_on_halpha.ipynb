{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is under construction. \n",
    "\n",
    "- In this notebook a rather simple 1-D CNN is trained and tested on H-alpha diagnostics. \n",
    "- Later this 1-D CNN may be used as a supplementary model for the ensembled RIS1xRIS2 resp. RIS1xRIS1 model, which have poor performance in distinguishing H-modes from ELMs.\n",
    "\n",
    "\n",
    "- Functions written here will migrate to `confinement_mode_classifier.py` once tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "import re\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "import pytorch_lightning as pl\n",
    "import confinement_mode_classifier as cmc\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, WeightedRandomSampler\n",
    "from torchmetrics.classification import MulticlassConfusionMatrix, F1Score, MulticlassPrecision, MulticlassRecall, MulticlassPrecisionRecallCurve, MulticlassROC\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from tempfile import TemporaryDirectory\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time \n",
    "from datetime import datetime\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(os.getcwd())\n",
    "data_dir_path = f'{path}/data/LH_alpha'\n",
    "file_names = os.listdir(data_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "#Time window for the diagnostics\n",
    "h_alpha_window = 50\n",
    "\n",
    "#Shots used in training\n",
    "shot_numbers = [re.search(r'shot_(\\d+)', file_name).group(1) for file_name in file_names]\n",
    "shots_for_testing = ['18130', '16773', '16534', \n",
    "                     '19094', '18133', '17837', \n",
    "                     '18128', '19915', '19925', \n",
    "                     '13182', '20009', '20112'\n",
    "                     ]\n",
    "\n",
    "shots_for_validation = ['16769', '19379', '18057', \n",
    "                        '18132', '18261', '18267', \n",
    "                        '18260', '20143', '20145', \n",
    "                        '20146', '20147', '20144', \n",
    "                        '20098'\n",
    "                        ]\n",
    "\n",
    "\n",
    "shot_df, test_df, val_df, train_df = cmc.load_and_split_dataframes(path, shot_numbers, \n",
    "                                                                   shots_for_testing, \n",
    "                                                                   shots_for_validation, \n",
    "                                                                   use_ELMS=True)\n",
    "\n",
    "#Test dloader is not balanced -> testing the ability to define ELM as anomalies\n",
    "test_dataloader = cmc.get_dloader(test_df, path=path, batch_size=batch_size, \n",
    "                                    balance_data=False, only_halpha=True, \n",
    "                                    second_img_opt=None, shuffle=False,\n",
    "                                    h_alpha_window = h_alpha_window)\n",
    "\n",
    "val_dataloader = cmc.get_dloader(val_df, path=path, batch_size=batch_size, \n",
    "                                    balance_data=True, only_halpha=True, \n",
    "                                    second_img_opt=None, shuffle=False,\n",
    "                                    h_alpha_window = h_alpha_window)\n",
    "\n",
    "train_dataloader = cmc.get_dloader(train_df, path=path, batch_size=batch_size, \n",
    "                                    balance_data=True, only_halpha=True, \n",
    "                                    second_img_opt=None, shuffle=False,\n",
    "                                    h_alpha_window = h_alpha_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class Simple1DCNN(nn.Module):\n",
    "    def __init__(self, num_classes=3, h_alpha_window=80):\n",
    "        super(Simple1DCNN, self).__init__()\n",
    "        # Define the 1D convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1, dilation=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=2)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(32)\n",
    "        # Define a fully connected layer for classification\n",
    "        ### in_features = floor[((input_length + 2*padding - dilation*(kernel_size - 1) - 1) // stride) + 1]\n",
    "        self.fc = nn.Linear(in_features=32 * (h_alpha_window - 2), out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply 1D convolutions\n",
    "        x = x.unsqueeze(1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.batch_norm1(x)  #!!! should I use some activation function here?\n",
    "\n",
    "        # Flatten the tensor for the fully connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Apply the fully connected layer and return the output\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This `train_model()` function is just a modified function copied from confinement_mode_classifier.py\n",
    "- The main difference resides in how the input data are parsed to the model (batch of all the diagnostics vs batch of just RIS imgs)\n",
    "- Will have to generalize all the models in order to use a single function for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler:lr_scheduler, dataloaders: dict,\n",
    "                 writer: SummaryWriter, dataset_sizes={'train':1, 'val':1}, num_epochs=25,\n",
    "                 chkpt_path=os.getcwd()):\n",
    "    since = time.time()\n",
    "\n",
    "\n",
    "    torch.save(model.state_dict(), chkpt_path)\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            num_of_samples = 0\n",
    "            running_batch = 0\n",
    "            # Iterate over data.\n",
    "            #TODO: eliminate the need in that dummy iterative for tensorboard part\n",
    "            for batch in tqdm(dataloaders[phase]):\n",
    "                \n",
    "                inputs = batch['h_alpha'].to(device).float() # #TODO: is it smart to convert double to float here? \n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                running_batch += 1\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs) #2D tensor with shape Batchsize*len(modes)\n",
    "                    #TODO: inputs.type. \n",
    "                    _, preds = torch.max(outputs, 1) #preds = 1D array of indicies of maximum values in row. ([2,1,2,1,2]) - third feature is largest in first sample, second in second...\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                num_of_samples += inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data) #How many correct answers\n",
    "                \n",
    "                \n",
    "                #tensorboard part\n",
    "                \n",
    "                if running_batch % int(len(dataloaders[phase])/10)==int(len(dataloaders[phase])/10)-1: \n",
    "                    # ...log the running loss\n",
    "                    \n",
    "                    #Training/validation loss\n",
    "                    writer.add_scalar(f'{phase}ing loss',\n",
    "                                    loss,\n",
    "                                    epoch * len(dataloaders[phase]) + running_batch)\n",
    "                    \n",
    "                    #F1 metric\n",
    "                    writer.add_scalar(f'{phase}ing F1 metric',\n",
    "                                    F1Score(task=\"multiclass\", num_classes=3).to(device)(preds, labels),\n",
    "                                    epoch * len(dataloaders[phase]) + running_batch)\n",
    "                    \n",
    "                    #Precision recall\n",
    "                    writer.add_scalar(f'{phase}ing macro Precision', \n",
    "                                        MulticlassPrecision(num_classes=3).to(device)(preds, labels),\n",
    "                                        epoch * len(dataloaders[phase]) + running_batch)\n",
    "                    \n",
    "                    writer.add_scalar(f'{phase}ing macro Recall', \n",
    "                                        MulticlassRecall(num_classes=3).to(device)(preds, labels),\n",
    "                                        epoch * len(dataloaders[phase]) + running_batch)\n",
    "                    \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                writer.add_scalar(f'best_accuracy for epoch',\n",
    "                                    epoch_acc,\n",
    "                                    epoch)\n",
    "                writer.close()\n",
    "                best_acc = epoch_acc\n",
    "                torch.save(model.state_dict(), chkpt_path)\n",
    "\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(chkpt_path))\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {'train':train_dataloader, 'val':val_dataloader}\n",
    "dataset_sizes = {x: len(dataloaders[x].dataset) for x in ['train', 'val']}\n",
    "\n",
    "timestamp =  datetime.fromtimestamp(time.time()).strftime(\"%y-%m-%d, %H-%M-%S \") + input('add comment: ')\n",
    "# create grid of images\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter(f'runs/{timestamp}')\n",
    "model_path = Path(f'{path}/runs/{timestamp}/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_cnn = Simple1DCNN(h_alpha_window=h_alpha_window)\n",
    "untrained_cnn = untrained_cnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = next(iter(train_dataloader))['h_alpha'].to(device).float()\n",
    "writer.add_graph(untrained_cnn, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = torch.optim.Adam(untrained_cnn.parameters(), lr=1e-3) #pouzit adam\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=1e-2, total_steps=50) #!!!\n",
    "\n",
    "num_epochs = 16\n",
    "trained_cnn = train_model(untrained_cnn, criterion, optimizer, exp_lr_scheduler, \n",
    "                       dataloaders, writer, dataset_sizes, num_epochs=num_epochs, \n",
    "                       chkpt_path = model_path.with_name(f'{model_path.stem}_chkpt{model_path.suffix}'))\n",
    "\n",
    "torch.save(trained_cnn.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Again, `test_model()` is modified function from `cmc` \n",
    "- Again main difference is how the batch is processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(run_path, model: torchvision.models.resnet.ResNet, test_dataloader: DataLoader,\n",
    "                max_batch: int = 0, return_metrics: bool = True, comment: str =''):\n",
    "    '''\n",
    "    Takes model and dataloader and returns figure with confusion matrix, \n",
    "    dataframe with predictions, F1 metric value, precision, recall and accuracy\n",
    "\n",
    "    Args:\n",
    "        model: ResNet model\n",
    "        test_dataloader: DataLoader used for testing\n",
    "        max_batch: maximum number of bathces to use for testing. Set = 0 to use all batches in DataLoader\n",
    "        return_metrics: if True returns confusion matrix, F1, precision, recall and accuracy \n",
    "    \n",
    "    Returns: \n",
    "        preds: pd.DataFrame() pd.DataFrame with columns of predicted class, true class, frame time and confidence of the prediction\n",
    "        precision: MulticlassPrecision(num_classes=3)\n",
    "        recall: MulticlassRecall(num_classes=3)\n",
    "        accuracy: (TP+TN)/(TP+TN+FN+FP)\n",
    "        fig_confusion_matrix: MulticlassConfusionMatrix(num_classes=3)\n",
    "    '''\n",
    "    y_df = torch.tensor([])\n",
    "    y_hat_df = torch.tensor([])\n",
    "    preds = pd.DataFrame(columns=['shot', 'prediction', 'label', 'time', 'confidence', 'L_logit', 'H_logit', 'ELM_logit'])\n",
    "    pattern = re.compile(r'RIS1_(\\d+)_t=')\n",
    "    batch_index = 0 #iterator\n",
    "    for batch in tqdm(test_dataloader, desc='Processing batches'):\n",
    "        batch_index +=1\n",
    "        outputs, y_hat, confidence = cmc.images_to_probs(model, batch['h_alpha'].to(device).float())\n",
    "        y_hat = torch.tensor(y_hat)\n",
    "        y_df = torch.cat((y_df.int(), batch['label']), dim=0)\n",
    "        y_hat_df = torch.cat((y_hat_df, y_hat), dim=0)\n",
    "        shot_numbers = [int(pattern.search(path).group(1)) for path in batch['path']]\n",
    "\n",
    "        pred = pd.DataFrame({'shot': shot_numbers, 'prediction': y_hat.data, \n",
    "                            'label': batch['label'].data, 'time':batch['time'], \n",
    "                            'confidence': confidence,'L_logit': outputs[:,0].cpu(), \n",
    "                            'H_logit': outputs[:,1].cpu(), 'ELM_logit': outputs[:,2].cpu()})\n",
    "\n",
    "        preds = pd.concat([preds, pred],axis=0, ignore_index=True)\n",
    "\n",
    "        if max_batch!=0 and batch_index>max_batch:\n",
    "            break\n",
    "\n",
    "    if return_metrics:\n",
    "        softmax_out = torch.nn.functional.softmax(torch.tensor(preds[['L_logit','H_logit','ELM_logit']].values), dim=1)\n",
    "        #Confusion matrix\n",
    "        confusion_matrix_metric = MulticlassConfusionMatrix(num_classes=3)\n",
    "        confusion_matrix_metric.update(y_hat_df, y_df)\n",
    "        conf_matrix_fig, conf_matrix_ax  = confusion_matrix_metric.plot()\n",
    "        #F1\n",
    "        f1 = F1Score(task=\"multiclass\", num_classes=3)(y_hat_df, y_df)\n",
    "\n",
    "        #Precision\n",
    "        precision = MulticlassPrecision(num_classes=3)(y_hat_df, y_df)\n",
    "        recall = MulticlassRecall(num_classes=3)(y_hat_df, y_df)\n",
    "        #precision(logits_df, y_df.int())\n",
    "         #Precision_recall curve\n",
    "        pr_curve = MulticlassPrecisionRecallCurve(num_classes=3, thresholds=64)\n",
    "        pr_curve.update(softmax_out, y_df)\n",
    "        pr_curve_fig, pr_curve_ax = pr_curve.plot(score=True)\n",
    "        #ROC metric\n",
    "        mcroc = MulticlassROC(num_classes=3, thresholds=64)\n",
    "        mcroc.update(torch.tensor(preds[['L_logit', 'H_logit', 'ELM_logit']].values.astype(float)), y_df)\n",
    "        roc_fig, roc_ax = mcroc.plot(score=True)\n",
    "        #Accuracy\n",
    "        accuracy = len(preds[preds['prediction']==preds['label']])/len(preds)\n",
    "\n",
    "        textstr = '\\n'.join((\n",
    "            f'Whole test dset',\n",
    "            r'threshhold = 0.5:',\n",
    "            r'f1=%.2f' % (f1.item(), ),\n",
    "            r'precision=%.2f' % (precision.item(), ),\n",
    "            r'recall=%.2f' % (recall.item(), ),\n",
    "            r'accuracy=%.2f' % (accuracy, )))\n",
    "        # these are matplotlib.patch.Patch properties\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        \n",
    "        conf_matrix_ax.set_title(f'confusion matrix for whole test dset')\n",
    "        pr_curve_ax.set_title(f'pr_curve for whole test dset')\n",
    "        pr_curve_ax.set_xlabel('Precision')\n",
    "        pr_curve_ax.set_ylabel('Recall')\n",
    "        roc_ax.text(0.05, 0.3, textstr, fontsize=14, verticalalignment='bottom', bbox=props)\n",
    "        roc_ax.set_xlabel('FP Rate')\n",
    "        roc_ax.set_ylabel('TP Rate')\n",
    "\n",
    "\n",
    "        # Open the saved images using Pillow\n",
    "        roc_img = cmc.matplotlib_figure_to_pil_image(roc_fig)\n",
    "        conf_matrix_img = cmc.matplotlib_figure_to_pil_image(conf_matrix_fig)\n",
    "        pr_curve_img = cmc.matplotlib_figure_to_pil_image(pr_curve_fig)\n",
    "        combined_image = Image.new('RGB', (conf_matrix_img.width + pr_curve_img.width + roc_img.width,\\\n",
    "                                            conf_matrix_img.height))\n",
    "\n",
    "        # Paste the saved images into the combined image\n",
    "        combined_image.paste(conf_matrix_img, (0, 0))\n",
    "        combined_image.paste(roc_img, (conf_matrix_img.width, 0))\n",
    "        combined_image.paste(pr_curve_img, (roc_img.width+conf_matrix_img.width, 0))\n",
    "        \n",
    "        # Save the combined image\n",
    "        combined_image.save(f'{run_path}/metrics_for_whole_test_dset_{comment}.png')\n",
    "\n",
    "        return preds, (conf_matrix_fig, conf_matrix_ax), f1, precision, recall, accuracy, (pr_curve_fig, pr_curve_ax), (roc_fig, roc_ax)\n",
    "    else: \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a5fdf5d63b48b1a341545065f952f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = test_model(f'{path}/runs/{timestamp}', trained_cnn, test_dataloader, comment ='3 classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_shot_test(path, shots: list, results_df: pd.DataFrame):\n",
    "    '''\n",
    "    Takes model's results dataframe from confinement_mode_classifier.test_model() and shot numbers.\n",
    "    Returns metrics of model for each shot separately\n",
    "\n",
    "    Args: \n",
    "        shots: list with numbers of shot to be tested on.\n",
    "        model: ResNet model\n",
    "        results_df: pd.DataFrame from confinement_mode_classifier.test_model().\n",
    "        time_confidence_img: Image with model confidence on separate shot\n",
    "        roc_img: Image with ROC \n",
    "        conf_matrix_img: Image with confusion matrix\n",
    "        combined_image: Combined image with three previous returns\n",
    "    Returns:\n",
    "        path: Path where images are saved\n",
    "    '''\n",
    "\n",
    "    for shot in tqdm(shots):\n",
    "        pred_for_shot = results_df[results_df['shot']==shot]\n",
    "        softmax_out = torch.nn.functional.softmax(torch.tensor(pred_for_shot[['L_logit','H_logit','ELM_logit']].values), dim=1)\n",
    "\n",
    "        preds_tensor = torch.tensor(pred_for_shot['prediction'].values.astype(float))\n",
    "        labels_tensor = torch.tensor(pred_for_shot['label'].values.astype(int))\n",
    "        \n",
    "        #Confusion matrix\n",
    "        confusion_matrix_metric = MulticlassConfusionMatrix(num_classes=3)\n",
    "        confusion_matrix_metric.update(preds_tensor, labels_tensor)\n",
    "        conf_matrix_fig, conf_matrix_ax = confusion_matrix_metric.plot()\n",
    "        \n",
    "\n",
    "        #f1 score\n",
    "        f1 = F1Score(task=\"multiclass\", num_classes=3)(preds_tensor, labels_tensor)\n",
    "\n",
    "        #Precision\n",
    "        precision = MulticlassPrecision(num_classes=3)(preds_tensor, labels_tensor)\n",
    "\n",
    "        #recall\n",
    "        recall = MulticlassRecall(num_classes=3)(preds_tensor, labels_tensor)\n",
    "\n",
    "        #accuracy\n",
    "        accuracy = len(pred_for_shot[pred_for_shot['prediction']==pred_for_shot['label']])/len(pred_for_shot)\n",
    "\n",
    "        textstr = '\\n'.join((\n",
    "            f'shot {shot}',\n",
    "            r'threshhold = 0.5:',\n",
    "            r'f1=%.2f' % (f1.item(), ),\n",
    "            r'precision=%.2f' % (precision.item(), ),\n",
    "            r'recall=%.2f' % (recall.item(), ),\n",
    "            r'accuracy=%.2f' % (accuracy, )))\n",
    "        # these are matplotlib.patch.Patch properties\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "        conf_time_fig, conf_time_ax = plt.subplots(figsize=(10,6))\n",
    "        conf_time_ax.plot(pred_for_shot['time'],softmax_out[:,1], label='H-mode Confidence')\n",
    "        conf_time_ax.plot(pred_for_shot['time'],-softmax_out[:,2], label='ELM Confidence')\n",
    "\n",
    "        conf_time_ax.scatter(pred_for_shot[pred_for_shot['label']==1]['time'], \n",
    "                          len(pred_for_shot[pred_for_shot['label']==1])*[1], \n",
    "                          s=2, alpha=1, label='H-mode Truth', color='maroon')\n",
    "        \n",
    "        conf_time_ax.scatter(pred_for_shot[pred_for_shot['label']==2]['time'], \n",
    "                          len(pred_for_shot[pred_for_shot['label']==2])*[-1], \n",
    "                          s=2, alpha=1, label='ELM Truth', color='royalblue')\n",
    "    \n",
    "        conf_time_ax.text(0.05, 0.3, textstr, fontsize=14, verticalalignment='bottom', bbox=props)\n",
    "        conf_time_ax.set_xlabel('t [ms]')\n",
    "        conf_time_ax.set_ylabel('Confidence')\n",
    "\n",
    "        plt.title(f'shot {shot}')\n",
    "        conf_time_ax.legend()\n",
    "\n",
    "        conf_matrix_ax.set_title(f'confusion matrix for shot {shot}')\n",
    "        conf_matrix_fig.set_figheight(conf_time_fig.get_size_inches()[1])\n",
    "\n",
    "        # Open the saved images using Pillow\n",
    "        time_confidence_img = matplotlib_figure_to_pil_image(conf_time_fig)\n",
    "        conf_matrix_img = matplotlib_figure_to_pil_image(conf_matrix_fig)\n",
    "\n",
    "        combined_image = Image.new('RGB', (time_confidence_img.width + conf_matrix_img.width,\n",
    "                                            time_confidence_img.height))\n",
    "\n",
    "        # Paste the saved images into the combined image\n",
    "        combined_image.paste(time_confidence_img, (0, 0))\n",
    "        combined_image.paste(conf_matrix_img, (time_confidence_img.width, 0))\n",
    "\n",
    "        # Save the combined image\n",
    "        combined_image.save(f'{path}/metrics_for_shot_{shot}.png')\n",
    "\n",
    "    return f'{path}/data'\n",
    "\n",
    "\n",
    "def matplotlib_figure_to_pil_image(fig):\n",
    "    \"\"\"\n",
    "    Convert a Matplotlib figure to a PIL Image.\n",
    "\n",
    "    Parameters:\n",
    "    - fig (matplotlib.figure.Figure): The Matplotlib figure to be converted.\n",
    "\n",
    "    Returns:\n",
    "    - PIL.Image.Image: The corresponding PIL Image.\n",
    "\n",
    "    Example:\n",
    "    >>> fig, ax = plt.subplots()\n",
    "    >>> ax.plot([1, 2, 3, 4], [10, 5, 20, 15])\n",
    "    >>> pil_image = matplotlib_figure_to_pil_image(fig)\n",
    "    >>> pil_image.save(\"output_image.png\")\n",
    "    >>> pil_image.show()\n",
    "    \"\"\"\n",
    "    # Create a FigureCanvasAgg to render the figure\n",
    "    canvas = FigureCanvasAgg(fig)\n",
    "\n",
    "    # Render the figure to a bitmap\n",
    "    canvas.draw()\n",
    "\n",
    "    # Get the RGB buffer from the bitmap\n",
    "    buf = canvas.buffer_rgba()\n",
    "\n",
    "    # Convert the buffer to a PIL Image\n",
    "    image = Image.frombuffer(\"RGBA\", canvas.get_width_height(), buf, \"raw\", \"RGBA\", 0, 1)\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ac4fa48ccc49aab487ee5d4aba9338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/compass/Shared/Users/bogdanov/vyzkumny_ukol/runs/24-02-21, 12-33-56 h_alpha, 1dCNN, lr=1e-3/data'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_shot_test(f'{path}/runs/{timestamp}', [int(shot) for shot in shots_for_testing], metrics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bogdanov_VU_kernel 3.8.10",
   "language": "python",
   "name": "bogdanov_vu_3.8.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
