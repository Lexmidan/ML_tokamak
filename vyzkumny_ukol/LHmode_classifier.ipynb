{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L-mode H-mode  Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import confinement_mode_classifier as cmc\n",
    "from datetime import datetime\n",
    "import time \n",
    "import torchvision\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(os.getcwd())\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "shots = [16534, 16769, 16773, 18130, 19237, 19240, 19379, 18057, 16989]\n",
    "shots_for_testing = [16769, 18130, 18057]\n",
    "shots_for_validation = [19237]\n",
    "\n",
    "shot_df, test_df, val_df, train_df = cmc.load_and_split_dataframes(path,shots, shots_for_testing, shots_for_validation )\n",
    "\n",
    "\n",
    "test_dataloader = cmc.get_dloader(test_df, path=path, batch_size=batch_size)\n",
    "val_dataloader = cmc.get_dloader(val_df, path=path, batch_size=batch_size)\n",
    "train_dataloader = cmc.get_dloader(train_df, path=path, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloader, that will be used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {'train':train_dataloader, 'val':val_dataloader}\n",
    "dataset_sizes = {x: len(dataloaders[x].dataset) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m timestamp \u001b[38;5;241m=\u001b[39m  datetime\u001b[38;5;241m.\u001b[39mfromtimestamp(time\u001b[38;5;241m.\u001b[39mtime())\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY, \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madd comment: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# create grid of images\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# default `log_dir` is \"runs\" - we'll be more specific here\u001b[39;00m\n\u001b[1;32m      5\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruns/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/compass/Shared/Users/bogdanov/.venv/lib/python3.8/site-packages/ipykernel/kernelbase.py:1251\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/compass/Shared/Users/bogdanov/.venv/lib/python3.8/site-packages/ipykernel/kernelbase.py:1295\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "timestamp =  datetime.fromtimestamp(time.time()).strftime(\"%d-%m-%Y, %H-%M-%S \") + input('add comment: ')\n",
    "\n",
    "# create grid of images\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter(f'runs/{timestamp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import ResNet pretrained model\n",
    "### And freeze all layers except last f.c. layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = torchvision.models.resnet18(weights='IMAGENET1K_V1', )\n",
    "for param in pretrained_model.parameters():\n",
    "    param.requires_grad = False\n",
    " \n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = pretrained_model.fc.in_features\n",
    "pretrained_model.fc = nn.Linear(num_ftrs, 3) #3 classes: L-mode, H-mode, ELM\n",
    "pretrained_model = pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = torch.optim.Adam(pretrained_model.parameters(), lr=0.001) #pouzit adam\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train last fc of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pretrained_model.load_state_dict(torch.load(f'{path}/runs/14-12-2023, 13-18-26 Compairing frozen and unfrozen models/model_fully_trained.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.eval()\n",
    "pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "#model = cmc.train_model(pretrained_model, criterion, optimizer, exp_lr_scheduler, \n",
    "#                        dataloaders, writer, dataset_sizes, num_epochs=num_epochs, comment = 'fc training')\n",
    "#model_path = Path(f'{path}/runs/{timestamp}/model_fc_trained.pt')\n",
    "#torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model with last fc trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df, fig_confusion_matrix, f1, precision, recall, accuracy = cmc.test_model(pretrained_model, test_dataloader, max_batch=50, return_metrics=True)\n",
    "\n",
    "writer.add_figure(f'Confusion matrix for the model with trained f.c. layer', fig_confusion_matrix)\n",
    "writer.add_scalar(f'Accuracy on test_dataset with trained f.c. layer', accuracy)\n",
    "writer.add_scalar(f'F1 metric on test_dataset with trained f.c. layer', f1)\n",
    "writer.add_scalar(f'Precision on test_dataset with trained f.c. layer', precision)\n",
    "writer.add_scalar(f'Recall on test_dataset with trained f.c. layer', recall)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>label</th>\n",
       "      <th>time</th>\n",
       "      <th>confidence</th>\n",
       "      <th>L_logit</th>\n",
       "      <th>H_logit</th>\n",
       "      <th>ELM_logit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1346.4</td>\n",
       "      <td>0.635283</td>\n",
       "      <td>2.086415</td>\n",
       "      <td>1.529145</td>\n",
       "      <td>-4.535882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1225.4</td>\n",
       "      <td>0.990333</td>\n",
       "      <td>3.746156</td>\n",
       "      <td>-0.924853</td>\n",
       "      <td>-4.082767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1169.8</td>\n",
       "      <td>0.993942</td>\n",
       "      <td>-0.205450</td>\n",
       "      <td>4.900466</td>\n",
       "      <td>-5.373708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1117.2</td>\n",
       "      <td>0.765530</td>\n",
       "      <td>1.140892</td>\n",
       "      <td>2.328135</td>\n",
       "      <td>-4.378273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>962.0</td>\n",
       "      <td>0.926161</td>\n",
       "      <td>2.759862</td>\n",
       "      <td>0.216888</td>\n",
       "      <td>-4.058347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1659</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1194.2</td>\n",
       "      <td>0.983276</td>\n",
       "      <td>0.162354</td>\n",
       "      <td>4.241520</td>\n",
       "      <td>-5.115597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1292.0</td>\n",
       "      <td>0.998257</td>\n",
       "      <td>4.687540</td>\n",
       "      <td>-1.734702</td>\n",
       "      <td>-4.336363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>973.8</td>\n",
       "      <td>0.962574</td>\n",
       "      <td>3.388908</td>\n",
       "      <td>0.133559</td>\n",
       "      <td>-4.677454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1132.2</td>\n",
       "      <td>0.610902</td>\n",
       "      <td>1.499335</td>\n",
       "      <td>1.953208</td>\n",
       "      <td>-4.389137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1176.8</td>\n",
       "      <td>0.993149</td>\n",
       "      <td>-0.174894</td>\n",
       "      <td>4.807387</td>\n",
       "      <td>-5.318163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1664 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prediction label    time  confidence   L_logit   H_logit  ELM_logit\n",
       "0             0     0  1346.4    0.635283  2.086415  1.529145  -4.535882\n",
       "1             0     0  1225.4    0.990333  3.746156 -0.924853  -4.082767\n",
       "2             1     1  1169.8    0.993942 -0.205450  4.900466  -5.373708\n",
       "3             1     1  1117.2    0.765530  1.140892  2.328135  -4.378273\n",
       "4             0     0   962.0    0.926161  2.759862  0.216888  -4.058347\n",
       "...         ...   ...     ...         ...       ...       ...        ...\n",
       "1659          1     1  1194.2    0.983276  0.162354  4.241520  -5.115597\n",
       "1660          0     0  1292.0    0.998257  4.687540 -1.734702  -4.336363\n",
       "1661          0     0   973.8    0.962574  3.388908  0.133559  -4.677454\n",
       "1662          1     1  1132.2    0.610902  1.499335  1.953208  -4.389137\n",
       "1663          1     1  1176.8    0.993149 -0.174894  4.807387  -5.318163\n",
       "\n",
       "[1664 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9099)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9148)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9097)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9098557692307693"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m      2\u001b[0m     param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m cmc\u001b[38;5;241m.\u001b[39mtrain_model(pretrained_model, criterion, optimizer, exp_lr_scheduler, \n\u001b[1;32m      4\u001b[0m                         dataloaders, writer, dataset_sizes, num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs, comment \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "model = cmc.train_model(pretrained_model, criterion, optimizer, exp_lr_scheduler, \n",
    "                        dataloaders, writer, dataset_sizes, num_epochs=num_epochs, comment = 'fc training')\n",
    "model_path = Path(f'{path}/runs/{timestamp}/model_fully_trained.pt')\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model with all layers trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, fig_confusion_matrix, f1, precision, recall, accuracy = cmc.test_model(model, test_dataloader, max_batch=50, return_metrics=True)\n",
    "\n",
    "writer.add_figure(f'Confusion matrix for the model with all layers trained', fig_confusion_matrix)\n",
    "writer.add_scalar(f'Accuracy on test_dataset with all layers trained', accuracy)\n",
    "writer.add_scalar(f'F1 metric on test_dataset with all layers trained', f1)\n",
    "writer.add_scalar(f'Precision on test_dataset with all layers trained', precision)\n",
    "writer.add_scalar(f'Recall on test_dataset with all layers trained', recall)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on time dependent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6278"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader.dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader_for_model_test = DataLoader(test_dataloader.dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictions_df_dtest, fig_confusion_matrix_dtest, f1_dtest, precision_dtest, recall_dtest, accuracy_dtest \u001b[38;5;241m=\u001b[39m \u001b[43mcmc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_for_model_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/compass/Shared/Users/bogdanov/vyzkumny_ukol/confinement_mode_classifier.py:336\u001b[0m, in \u001b[0;36mtest_model\u001b[0;34m(model, test_dataloader, max_batch, return_metrics)\u001b[0m\n\u001b[1;32m    333\u001b[0m preds \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL_logit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mH_logit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mELM_logit\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    334\u001b[0m pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRIS1_(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)_t=\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 336\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_index, (img, y, paths, times) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_dataloader):\n\u001b[1;32m    337\u001b[0m     outputs, y_hat, confidence \u001b[38;5;241m=\u001b[39m images_to_probs(model,img\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m    338\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_hat)\n",
      "File \u001b[0;32m/compass/Shared/Users/bogdanov/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/compass/Shared/Users/bogdanov/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/compass/Shared/Users/bogdanov/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/compass/Shared/Users/bogdanov/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/compass/Shared/Users/bogdanov/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/compass/Shared/Users/bogdanov/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/compass/Shared/Users/bogdanov/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/compass/Shared/Users/bogdanov/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictions_df_dtest, fig_confusion_matrix_dtest, f1_dtest, precision_dtest, recall_dtest, accuracy_dtest = cmc.test_model(pretrained_model, dataloader_for_model_test,max_batch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>mode</th>\n",
       "      <th>filename</th>\n",
       "      <th>shot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4359</th>\n",
       "      <td>960.4</td>\n",
       "      <td>0</td>\n",
       "      <td>imgs/RIS1_16773_t=960.4.png</td>\n",
       "      <td>16773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4360</th>\n",
       "      <td>960.6</td>\n",
       "      <td>0</td>\n",
       "      <td>imgs/RIS1_16773_t=960.6.png</td>\n",
       "      <td>16773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4361</th>\n",
       "      <td>960.8</td>\n",
       "      <td>0</td>\n",
       "      <td>imgs/RIS1_16773_t=960.8.png</td>\n",
       "      <td>16773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4362</th>\n",
       "      <td>961.0</td>\n",
       "      <td>0</td>\n",
       "      <td>imgs/RIS1_16773_t=961.0.png</td>\n",
       "      <td>16773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4363</th>\n",
       "      <td>961.2</td>\n",
       "      <td>0</td>\n",
       "      <td>imgs/RIS1_16773_t=961.2.png</td>\n",
       "      <td>16773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6534</th>\n",
       "      <td>1395.4</td>\n",
       "      <td>0</td>\n",
       "      <td>imgs/RIS1_16773_t=1395.4.png</td>\n",
       "      <td>16773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6535</th>\n",
       "      <td>1395.6</td>\n",
       "      <td>0</td>\n",
       "      <td>imgs/RIS1_16773_t=1395.6.png</td>\n",
       "      <td>16773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6536</th>\n",
       "      <td>1395.8</td>\n",
       "      <td>0</td>\n",
       "      <td>imgs/RIS1_16773_t=1395.8.png</td>\n",
       "      <td>16773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6537</th>\n",
       "      <td>1396.0</td>\n",
       "      <td>0</td>\n",
       "      <td>imgs/RIS1_16773_t=1396.0.png</td>\n",
       "      <td>16773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6538</th>\n",
       "      <td>1396.2</td>\n",
       "      <td>0</td>\n",
       "      <td>imgs/RIS1_16773_t=1396.2.png</td>\n",
       "      <td>16773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2180 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time mode                      filename   shot\n",
       "4359   960.4    0   imgs/RIS1_16773_t=960.4.png  16773\n",
       "4360   960.6    0   imgs/RIS1_16773_t=960.6.png  16773\n",
       "4361   960.8    0   imgs/RIS1_16773_t=960.8.png  16773\n",
       "4362   961.0    0   imgs/RIS1_16773_t=961.0.png  16773\n",
       "4363   961.2    0   imgs/RIS1_16773_t=961.2.png  16773\n",
       "...      ...  ...                           ...    ...\n",
       "6534  1395.4    0  imgs/RIS1_16773_t=1395.4.png  16773\n",
       "6535  1395.6    0  imgs/RIS1_16773_t=1395.6.png  16773\n",
       "6536  1395.8    0  imgs/RIS1_16773_t=1395.8.png  16773\n",
       "6537  1396.0    0  imgs/RIS1_16773_t=1396.0.png  16773\n",
       "6538  1396.2    0  imgs/RIS1_16773_t=1396.2.png  16773\n",
       "\n",
       "[2180 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2180"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df = shot_df[shot_df['shot']==16773].reset_index(drop=True)\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "dset_for_testing = cmc.ImageDataset(df, path, mean=mean, std=std)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c0fcb9a8ee4f72b0de915adb9bb100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "shots_for_testing = [16773]\n",
    "for shot in tqdm(shots_for_testing):\n",
    "    df = shot_df[shot_df['shot']==shot].reset_index(drop=True)\n",
    "    dset_for_testing = cmc.ImageDataset(df, path, mean=mean, std=std)\n",
    "    dloader = DataLoader(dset_for_testing, batch_size=batch_size, shuffle=False)\n",
    "    predictions_df_dtest, fig_confusion_matrix_dtest, f1_dtest, precision_dtest, recall_dtest, accuracy_dtest = cmc.test_model(\n",
    "        pretrained_model, dloader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>mode</th>\n",
       "      <th>filename</th>\n",
       "      <th>shot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2181</th>\n",
       "      <td>960.8</td>\n",
       "      <td>0</td>\n",
       "      <td>imgs/RIS1_16769_t=960.8.png</td>\n",
       "      <td>16769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2182</th>\n",
       "      <td>961.0</td>\n",
       "      <td>0</td>\n",
       "      <td>imgs/RIS1_16769_t=961.0.png</td>\n",
       "      <td>16769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2183</th>\n",
       "      <td>961.2</td>\n",
       "      <td>0</td>\n",
       "      <td>imgs/RIS1_16769_t=961.2.png</td>\n",
       "      <td>16769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>961.4</td>\n",
       "      <td>0</td>\n",
       "      <td>imgs/RIS1_16769_t=961.4.png</td>\n",
       "      <td>16769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185</th>\n",
       "      <td>961.6</td>\n",
       "      <td>0</td>\n",
       "      <td>imgs/RIS1_16769_t=961.6.png</td>\n",
       "      <td>16769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16490</th>\n",
       "      <td>1369.0</td>\n",
       "      <td>0</td>\n",
       "      <td>imgs/RIS1_18057_t=1369.0.png</td>\n",
       "      <td>18057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16491</th>\n",
       "      <td>1369.2</td>\n",
       "      <td>0</td>\n",
       "      <td>imgs/RIS1_18057_t=1369.2.png</td>\n",
       "      <td>18057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16492</th>\n",
       "      <td>1369.4</td>\n",
       "      <td>0</td>\n",
       "      <td>imgs/RIS1_18057_t=1369.4.png</td>\n",
       "      <td>18057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16493</th>\n",
       "      <td>1369.6</td>\n",
       "      <td>0</td>\n",
       "      <td>imgs/RIS1_18057_t=1369.6.png</td>\n",
       "      <td>18057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16494</th>\n",
       "      <td>1369.8</td>\n",
       "      <td>0</td>\n",
       "      <td>imgs/RIS1_18057_t=1369.8.png</td>\n",
       "      <td>18057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6278 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         time mode                      filename   shot\n",
       "2181    960.8    0   imgs/RIS1_16769_t=960.8.png  16769\n",
       "2182    961.0    0   imgs/RIS1_16769_t=961.0.png  16769\n",
       "2183    961.2    0   imgs/RIS1_16769_t=961.2.png  16769\n",
       "2184    961.4    0   imgs/RIS1_16769_t=961.4.png  16769\n",
       "2185    961.6    0   imgs/RIS1_16769_t=961.6.png  16769\n",
       "...       ...  ...                           ...    ...\n",
       "16490  1369.0    0  imgs/RIS1_18057_t=1369.0.png  18057\n",
       "16491  1369.2    0  imgs/RIS1_18057_t=1369.2.png  18057\n",
       "16492  1369.4    0  imgs/RIS1_18057_t=1369.4.png  18057\n",
       "16493  1369.6    0  imgs/RIS1_18057_t=1369.6.png  18057\n",
       "16494  1369.8    0  imgs/RIS1_18057_t=1369.8.png  18057\n",
       "\n",
       "[6278 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shots_for_testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6278"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader.dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile(r'RIS1_(\\d+)_t=')\n",
    "\n",
    "# Extract shot numbers using a list comprehension\n",
    "shot_numbers = [int(pattern.search(path).group(1)) for path in next(iter(test_dataloader))[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18057,\n",
       " 16769,\n",
       " 18130,\n",
       " 18130,\n",
       " 16769,\n",
       " 18057,\n",
       " 18130,\n",
       " 18057,\n",
       " 18130,\n",
       " 18057,\n",
       " 18057,\n",
       " 18130,\n",
       " 16769,\n",
       " 18130,\n",
       " 18130,\n",
       " 16769,\n",
       " 18130,\n",
       " 16769,\n",
       " 18057,\n",
       " 18057,\n",
       " 18057,\n",
       " 18130,\n",
       " 18130,\n",
       " 18057,\n",
       " 18130,\n",
       " 18057,\n",
       " 18057,\n",
       " 18130,\n",
       " 18130,\n",
       " 16769,\n",
       " 18130,\n",
       " 18130]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shot_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = next(iter(test_dataloader))[0].to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.7360,  4.4382,  9.7772,  5.6418, -0.9460,  2.1114, -1.4919,  5.9128,\n",
       "         5.2321,  2.4490,  5.7163,  9.6211, -1.5153, -0.8785,  0.4527,  9.1658,\n",
       "         0.0572,  4.9005, -1.4994,  5.9457,  3.7803,  4.4129,  0.5173,  0.9920,\n",
       "         4.7426, -1.4576, -1.1690,  5.3980, -1.4972,  5.5125, -1.0812,  9.0756],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmc.images_to_probs(pretrained_model, test_imgs)[0][:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 8952, 'val': 1952}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7665aab2eee4218988335df1c09e818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "a = torch.tensor([])\n",
    "for batch in tqdm(test_dataloader):\n",
    "    a = torch.cat([a, batch[-1]])\n",
    "    if len(a)>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1140.6000, 1142.4000, 1257.8000, 1131.6000, 1131.4000, 1054.6000,\n",
       "        1126.4000, 1139.8000, 1025.8000, 1378.8000, 1195.4000, 1017.6000,\n",
       "        1356.4000,  962.2000, 1011.0000, 1172.0000, 1141.0000, 1141.4000,\n",
       "        1383.2000, 1101.8000, 1314.8000, 1225.0000,  963.0000,  981.4000,\n",
       "        1366.6000, 1132.4000, 1002.2000, 1154.6000, 1335.6000, 1006.8000,\n",
       "        1020.2000, 1166.8000], dtype=torch.float64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0088,  0.0259, -0.0083,  ..., -0.0083, -0.0083, -0.0083],\n",
       "          [-0.0083,  0.0088,  0.0259,  ..., -0.0083, -0.0083, -0.0083],\n",
       "          [-0.0083, -0.0083, -0.0083,  ..., -0.0083,  0.0088,  0.0259],\n",
       "          ...,\n",
       "          [-0.0083, -0.0083, -0.0083,  ...,  0.0431,  0.0088, -0.0083],\n",
       "          [-0.0083, -0.0083, -0.0083,  ..., -0.0083, -0.0083, -0.0083],\n",
       "          [-0.0083, -0.0083, -0.0083,  ...,  0.0259, -0.0083,  0.0088]],\n",
       " \n",
       "         [[-0.0080, -0.0080,  0.0095,  ..., -0.0080, -0.0080, -0.0080],\n",
       "          [ 0.0095, -0.0080,  0.0095,  ..., -0.0080, -0.0080, -0.0080],\n",
       "          [-0.0080,  0.0095, -0.0080,  ..., -0.0080, -0.0080, -0.0080],\n",
       "          ...,\n",
       "          [-0.0080, -0.0080, -0.0080,  ...,  0.0095,  0.0270, -0.0080],\n",
       "          [-0.0080, -0.0080,  0.0095,  ...,  0.0095, -0.0080, -0.0080],\n",
       "          [-0.0080, -0.0080, -0.0080,  ..., -0.0080, -0.0080,  0.0095]],\n",
       " \n",
       "         [[-0.0071,  0.0104, -0.0071,  ..., -0.0071, -0.0071, -0.0071],\n",
       "          [-0.0071, -0.0071,  0.0104,  ...,  0.0104,  0.0104, -0.0071],\n",
       "          [-0.0071, -0.0071, -0.0071,  ...,  0.0104, -0.0071, -0.0071],\n",
       "          ...,\n",
       "          [-0.0071, -0.0071, -0.0071,  ...,  0.0278,  0.0278,  0.0104],\n",
       "          [ 0.0104, -0.0071, -0.0071,  ..., -0.0071, -0.0071,  0.0104],\n",
       "          [-0.0071, -0.0071, -0.0071,  ..., -0.0071,  0.0104, -0.0071]]],\n",
       "        dtype=torch.float64),\n",
       " 0,\n",
       " '/compass/Shared/Users/bogdanov/vyzkumny_ukol/imgs/RIS1_16769_t=961.4.png',\n",
       " 961.4)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader.dataset.__getitem__(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bogdanov_VU_kernel 3.8.10",
   "language": "python",
   "name": "bogdanov_vu_3.8.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
